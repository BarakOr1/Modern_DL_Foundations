{"cells":[{"cell_type":"markdown","source":["This notebook demonstrates how to build, train, and evaluate a simple Autoencoder on the MNIST dataset of handwritten digits. The Autoencoder compresses images into a 2-dimensional bottleneck representation and then reconstructs them back to their original form.\n","\n","Key steps in this notebook:\n","\n","* Define an Autoencoder model with an encoder (reduces input size to a 2D latent space) and a decoder (reconstructs the image).\n","\n","* Train the model using mean squared error (MSE) as the loss function and the Adam optimizer.\n","\n","* Visualize the latent space by plotting the 2D representations of all digits, colored by their labels.\n","\n","* Compare original images with their reconstructions to evaluate model performance.\n","\n","At the end of the notebook, you will also find short reflection questions to think about the role of activation functions, bottleneck size, and loss reduction during training.\n"],"metadata":{"id":"9HRaJxYF6H3U"}},{"cell_type":"code","source":["# Install libraries (if needed)\n","!pip install torch torchvision matplotlib\n","\n","# Import modules\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1EdXw2N6AaF","executionInfo":{"status":"ok","timestamp":1758385560083,"user_tz":-180,"elapsed":5440,"user":{"displayName":"Barak Or","userId":"02488591995076339770"}},"outputId":"1da707d1-f14a-43f1-8ea5-f058fc0103f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"]}]},{"cell_type":"markdown","source":["1. Define the bottleneck size (currently set to 2).\n","2. Use a suitable activation function for the output layer to ensure pixel values are in the correct range."],"metadata":{"id":"GQFxm1Ih7MXU"}},{"cell_type":"code","source":["# Define an Autoencoder class by extending nn.Module\n","class Autoencoder(nn.Module):\n","    def __init__(self):\n","        # Initialize the parent class (nn.Module)\n","        super().__init__()\n","\n","        # -------------------------\n","        # Encoder: compresses input\n","        # -------------------------\n","        self.encoder = nn.Sequential(\n","            nn.Flatten(),               # Flatten 28x28 image → vector of size 784\n","            nn.Linear(28*28, 128),      # Fully connected layer: 784 → 128 features\n","            nn.ReLU(),                  # Non-linear activation (introduces non-linearity)\n","            nn.Linear(128, ???)           # Bottleneck layer: compress to 2 dimensions\n","        )\n","\n","        # -------------------------\n","        # Decoder: reconstructs input\n","        # -------------------------\n","        self.decoder = nn.Sequential(\n","            nn.Linear(???, 128),          # Expand back from 2 → 128 features\n","            nn.ReLU(),                  # Non-linear activation\n","            nn.Linear(128, 28*28),      # Map 128 features back to 784 pixels\n","            nn.???(),               # Ensure outputs are between 0–1 (pixel range)\n","            nn.Unflatten(1, (1, 28, 28))# Reshape vector back into image: (1, 28, 28)\n","        )\n","\n","    # Forward pass: how data flows through the network\n","    def forward(self, x):\n","        z = self.encoder(x)      # Encode input into 2D latent representation\n","        x_hat = self.decoder(z)  # Decode latent representation back to image\n","        return x_hat, z          # Return reconstructed image and latent vector"],"metadata":{"id":"IaqqzMzl6QRc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load MNIST dataset\n","transform = transforms.ToTensor()\n","mnist = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform)\n","dataloader = DataLoader(mnist, batch_size=256, shuffle=True)"],"metadata":{"id":"ATj24B9B6UBZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Complete the setup for training the Autoencoder by defining:\n","* the loss function suitable for image reconstruction.\n","* The optimizer to update model weights (options: optim.SGD, optim.Adam, optim.RMSprop) with a learning rate of 1e-3."],"metadata":{"id":"V-KTke4U7fpS"}},{"cell_type":"code","source":["# Model, loss function, and optimizer\n","model = Autoencoder()\n","criterion = ???\n","optimizer = ???"],"metadata":{"id":"EzbbroyD6bXO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop for the Autoencoder\n","# Note: this is a simple demonstration and not a fully optimized setup\n","\n","# We will train for 50 epochs (passes through the entire dataset)\n","for epoch in range(50):\n","    # Initialize a variable to accumulate the total loss for this epoch\n","    total_loss = 0\n","\n","    # Loop through the dataset in batches\n","    # 'images' contains the input batch, '_' ignores the labels (not needed for autoencoders)\n","    for images, _ in dataloader:\n","\n","        # Forward pass: pass images through the model\n","        # x_hat = reconstructed images, z = latent representation (ignored here)\n","        x_hat, _ = model(images)\n","\n","        # Compute the loss: how different are reconstructed images from the originals?\n","        # Using Mean Squared Error (MSE) between x_hat and images\n","        loss = criterion(x_hat, images)\n","\n","        # Reset gradients before backpropagation\n","        # Otherwise, PyTorch would accumulate gradients from previous steps\n","        optimizer.zero_grad()\n","\n","        # Backward pass: compute gradients of loss w.r.t. all trainable parameters\n","        loss.backward()\n","\n","        # Update parameters using the chosen optimizer (Adam in this case)\n","        optimizer.step()\n","\n","        # Accumulate the loss for monitoring\n","        total_loss += loss.item()\n","\n","    # Print the total loss after one epoch (all batches)\n","    # This gives an indication of how well the model is learning\n","    print(f\"Epoch {epoch+1}, Loss: {total_loss:.2f}\")"],"metadata":{"id":"7fVIBA-L-gLC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualization of the latent space learned by the Autoencoder:"],"metadata":{"id":"47UKxAbA9o_D"}},{"cell_type":"code","source":["# Plot latent space\n","model.eval()\n","all_z = []\n","all_labels = []\n","with torch.no_grad():\n","    for images, labels in DataLoader(mnist, batch_size=512):\n","        _, z = model(images)\n","        all_z.append(z)\n","        all_labels.append(labels)\n","z_all = torch.cat(all_z).numpy()\n","labels_all = torch.cat(all_labels).numpy()\n","\n","\n","plt.figure(figsize=(8, 6))\n","scatter = plt.scatter(z_all[:, 0], z_all[:, 1], c=labels_all, cmap='tab10', alpha=0.7, s=15)\n","plt.colorbar(scatter, ticks=range(10))\n","plt.title(\"2D Bottleneck Representation (Autoencoder on MNIST)\")\n","plt.xlabel(\"z1\")\n","plt.ylabel(\"z2\")\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"CPTO9ny--j4L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["10 original vs reconstructed images"],"metadata":{"id":"XdlMssje9tm5"}},{"cell_type":"code","source":["images, _ = next(iter(DataLoader(mnist, batch_size=10)))\n","with torch.no_grad():\n","    recon, _ = model(images)\n","\n","fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n","for i in range(10):\n","    axes[0, i].imshow(images[i][0], cmap='gray')\n","    axes[0, i].axis('off')\n","    axes[1, i].imshow(recon[i][0], cmap='gray')\n","    axes[1, i].axis('off')\n","\n","axes[0, 0].set_ylabel(\"Original\", fontsize=12)\n","axes[1, 0].set_ylabel(\"Reconstructed\", fontsize=12)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"eIA4Uial-mOl"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyPLr2MDI6n5phmYNh1Ps2/5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}