{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNyqgIvb+T0I27p/DOoewcc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":[" # Text Classification with RNN / LSTM / GRU Using Reuters Dataset"],"metadata":{"id":"ZjXhfBGiAq-F"}},{"cell_type":"markdown","source":["In this exercise, you will build and compare text classification models using the Reuters newswire dataset, a well-known benchmark for multi-class topic classification. You will learn how to preprocess text data, represent it using embeddings, and train recurrent neural networks - including Simple RNN, LSTM, and GRU - to identify news topics automatically.\n","\n","By completing this notebook, you will gain practical experience in:\n","\n","Preparing textual data for deep learning models.\n","\n","Understanding the differences between RNN, LSTM, and GRU architectures.\n","\n","Evaluating and visualizing model performance on real-world data.\n","\n","This exercise helps develop intuition for how recurrent models capture sequential patterns in language and how architectural choices affect accuracy and generalization."],"metadata":{"id":"4i03Aj4rAkNq"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","\n","# Set random seed for reproducibility\n","SEED = 42\n","tf.keras.utils.set_random_seed(SEED)"],"metadata":{"id":"oSrhLQY0A0g3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Load Reuters dataset\n","The Reuters dataset contains newswire topics classification (46 classes). We keep only the top 'num_words' most frequent words, and cut sequences to 'maxlen' tokens.\n","Define num_words to be 5,000 and the max_len to ne 100."],"metadata":{"id":"75KHnbOJBUSR"}},{"cell_type":"code","source":["num_words = ???\n","maxlen = ???\n","\n","(x_train, y_train), (x_test, y_test) = keras.datasets.reuters.load_data(num_words=num_words)\n","\n","# Pad sequences to ensure they all have the same length\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test  = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n","\n","# Split off a validation set (first 1000 samples)\n","x_val, y_val = x_train[:1000], y_train[:1000]\n","x_train, y_train = x_train[1000:], y_train[1000:]\n","\n","print(\"Train:\", x_train.shape, \"Val:\", x_val.shape, \"Test:\", x_test.shape)\n","\n","# Number of output classes\n","num_classes = max(y_train) + 1\n","print(\"Num classes:\", num_classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lgVvFtZkA-d4","executionInfo":{"status":"ok","timestamp":1759779377499,"user_tz":-180,"elapsed":2877,"user":{"displayName":"Barak Or","userId":"02488591995076339770"}},"outputId":"f52cbcbb-cc2e-4595-91e4-724dceb9830b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n","\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n","Train: (7982, 100) Val: (1000, 100) Test: (2246, 100)\n","Num classes: 46\n"]}]},{"cell_type":"markdown","source":["# 2. Build model function\n","* Define a dense (fully connected) hidden layer in the model. The layer should have 64 units and use the GELU activation function.\n","* Following the layer with a Dropout for regularization (prevents overfitting), parameter=0.5."],"metadata":{"id":"u4955UspBdyn"}},{"cell_type":"code","source":["def build_model(cell_type=\"RNN\", units=32, vocab_size=5000, maxlen=100, num_classes=46):\n","\n","    # Input layer for padded sequences\n","    inputs = keras.Input(shape=(maxlen,))\n","\n","    # Embedding layer: turns integer word indices into dense vectors\n","    x = layers.Embedding(input_dim=vocab_size, output_dim=64)(inputs)\n","\n","    # Choose the recurrent layer\n","    if cell_type == \"RNN\":\n","        x = layers.SimpleRNN(units)(x)\n","    elif cell_type == \"LSTM\":\n","        x = layers.LSTM(units)(x)\n","    elif cell_type == \"GRU\":\n","        x = layers.GRU(units)(x)\n","    else:\n","        raise ValueError(\"Unknown cell type\")\n","\n","    # Add a fully connected hidden layer for extra learning capacity\n","    x =???\n","\n","    # Add Dropout for regularization\n","    x = ???\n","\n","    # Output layer: multi-class classification with softmax\n","    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n","\n","    # Build and compile the model\n","    model = keras.Model(inputs, outputs, name=f\"{cell_type}_u{units}\")\n","    model.compile(\n","        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n","        loss=\"sparse_categorical_crossentropy\",\n","        metrics=[\"accuracy\"]\n","    )\n","    return model"],"metadata":{"id":"-QQVADEKBBrm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Train models\n","We will compare different architectures (RNN, LSTM, GRU) with 32 and 64 units\n","* Complete the epochs and batch size (i.e. 20 and 128, respectively)."],"metadata":{"id":"BRUwRxGIBi4B"}},{"cell_type":"code","source":["configs = [\n","    (\"RNN\", 32),\n","    (\"RNN\", 64),\n","    (\"LSTM\", 32),\n","    (\"LSTM\", 64),\n","    (\"GRU\", 32),\n","    (\"GRU\", 64),\n","]\n","\n","histories = {}\n","for cell, units in configs:\n","    print(f\"\\nTraining {cell} with {units} units\")\n","    model = build_model(cell, units, num_words, maxlen, num_classes)\n","\n","    # Train with validation\n","    hist = model.fit(\n","        x_train, y_train,\n","        validation_data=(x_val, y_val),\n","        epochs=???,\n","        batch_size=???,\n","        verbose=1\n","    )\n","\n","    # Evaluate on the test set\n","    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n","    print(f\"Test accuracy: {test_acc:.3f}\")\n","\n","    histories[f\"{cell}_u{units}\"] = hist"],"metadata":{"id":"5VHJHGTgx6Px"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Plot learning curves"],"metadata":{"id":"Rydq_EQ1BoEB"}},{"cell_type":"code","source":["# Plot Validation Accuracy\n","plt.figure(figsize=(12,5))\n","for name, hist in histories.items():\n","    plt.plot(hist.history[\"val_accuracy\"], label=name)\n","plt.title(\"Validation Accuracy: RNN vs LSTM vs GRU (Reuters)\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Val Accuracy\")\n","plt.legend()\n","plt.show()\n","\n","# Plot Validation Loss\n","plt.figure(figsize=(12,5))\n","for name, hist in histories.items():\n","    plt.plot(hist.history[\"val_loss\"], label=name)\n","plt.title(\"Validation Loss: RNN vs LSTM vs GRU (Reuters)\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Val Loss\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"dyhkCkv3x9c7"},"execution_count":null,"outputs":[]}]}