{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyPZOnGheVc+Jr3iiyKpBUf0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["In this exercise, you will explore three common strategies for applying Transfer Learning using a pretrained ResNet18 model on the CIFAR-10 dataset. The goal is to compare how much of the pretrained knowledge from ImageNet should be reused versus fine-tuned for a new task.\n","You will implement and evaluate:\n","\n","Feature Extraction (Fixed Base) – freeze all pretrained layers and train only the final classifier.\n","\n","Partial Fine-Tuning – unfreeze the deeper layers (e.g., layer4 and fc) to adapt high-level features.\n","\n","Full Fine-Tuning – train all layers with a lower learning rate for full adaptation.\n","\n","The script trains each variant for a few epochs, measures test accuracy, and prints a summary comparison of the three approaches.\n","\n"],"metadata":{"id":"8-mGb7MuVM8I"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"oB_UCFfgVSyr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imagenet_mean = [0.485, 0.456, 0.406]\n","imagenet_std  = [0.229, 0.224, 0.225]\n","\n","transform_train = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(imagenet_mean, imagenet_std)\n","])\n","transform_test = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(imagenet_mean, imagenet_std)\n","])\n","\n","train_data = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform_train)\n","test_data  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=transform_test)\n","train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n","test_loader  = DataLoader(test_data, batch_size=128, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I34nTzZyVWf_","executionInfo":{"status":"ok","timestamp":1761210996888,"user_tz":-180,"elapsed":5243,"user":{"displayName":"Barak Or","userId":"02488591995076339770"}},"outputId":"d3b10145-6274-4f9c-c20c-70972fc4f63a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:01<00:00, 86.7MB/s]\n"]}]},{"cell_type":"code","source":["def train_model(model, optimizer, criterion, epochs=3):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0\n","        for x, y in train_loader:\n","            x, y = x.to(device), y.to(device)\n","            optimizer.zero_grad()\n","            loss = criterion(model(x), y)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        print(f\"Epoch {epoch+1}/{epochs}  Loss: {running_loss/len(train_loader):.4f}\")\n","\n","def evaluate(model):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in test_loader:\n","            x, y = x.to(device), y.to(device)\n","            preds = model(x).argmax(1)\n","            correct += (preds == y).sum().item()\n","            total += y.size(0)\n","    return correct / total"],"metadata":{"id":"lFPcNg42VdT4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Strategy 1 – Feature Extraction\n","\n","Train the model using feature extraction and explain how freezing pretrained layers affects its ability to learn the new CIFAR-10 classes."],"metadata":{"id":"0H7sQHacVf68"}},{"cell_type":"code","source":["model = models.resnet18(weights=\"IMAGENET1K_V1\")\n","for p in model.parameters():\n","    p.requires_grad = False\n","model.fc = nn.Linear(512, 10)\n","model = model.to(device)\n","\n","optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","print(\"\\n=== Feature Extraction ===\")\n","train_model(model, optimizer, criterion, epochs=3)\n","acc_fixed = evaluate(model)\n","print(f\"Feature Extraction Accuracy: {acc_fixed:.3f}\")"],"metadata":{"id":"MsQNp7AXoR0U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Strategy 2 – Partial Fine-Tuning\n","\n","Complete and run the code to fine-tune only the layer4 and fc layers for 3 epochs, then write down the final accuracy (acc_partial) and explain whether partial fine-tuning improved the results compared to full freezing."],"metadata":{"id":"o3cCeucgVnv3"}},{"cell_type":"code","source":["model = models.resnet18(weights=\"IMAGENET1K_V1\")\n","for name, p in model.named_parameters():\n","    p.requires_grad = (\"???\" in name) or (\"???\" in name)\n","model.fc = nn.Linear(512, 10)\n","model = model.to(device)\n","\n","optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","print(\"\\n=== Partial Fine-Tuning ===\")\n","train_model(model, optimizer, criterion, epochs=???)\n","acc_partial = evaluate(model)\n","print(f\"Partial Fine-Tuning Accuracy: {acc_partial:.3f}\")"],"metadata":{"id":"0S5CNefxofNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Strategy 3 – Full Fine-Tuning\n","\n","Run the code to fully fine-tune all layers of ResNet18 for 3 epochs, record the final accuracy (acc_full), and compare the result to the previous two strategies to determine which transfer learning method performs best."],"metadata":{"id":"afreJIHuVuRZ"}},{"cell_type":"code","source":["model = models.resnet18(weights=\"IMAGENET1K_V1\")\n","for p in model.parameters():\n","    p.requires_grad = True\n","model.fc = nn.Linear(512, 10)\n","model = model.to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=1e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","print(\"\\n=== Full Fine-Tuning ===\")\n","train_model(model, optimizer, criterion, epochs=???)\n","acc_full = evaluate(model)\n","print(f\"Full Fine-Tuning Accuracy: {acc_full:.3f}\")"],"metadata":{"id":"OW6K0J2hom4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n---------------------------\")\n","print(f\"Feature Extraction:  {acc_fixed:.3f}\")\n","print(f\"Partial Fine-Tuning: {acc_partial:.3f}\")\n","print(f\"Full Fine-Tuning:    {acc_full:.3f}\")\n","print(\"---------------------------\")"],"metadata":{"id":"hqrZMBZvoo-W"},"execution_count":null,"outputs":[]}]}